{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import torch\n",
    "import tiktoken\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text file\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding the text and cleaning it\n",
    "file_path = 'training_text.txt'\n",
    "text = read_text_file(file_path)\n",
    "\n",
    "text = text.strip(\"\\ufeff\")\n",
    "text = text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270151\n",
      "In my younger and more vulnerable years my father gave me some advice that I’ve been turning over in\n"
     ]
    }
   ],
   "source": [
    "# length of text\n",
    "print(len(text))\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !$()*,-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXY[]abcdefghijklmnopqrstuvwxyzçéêô —‘’“”…\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "# checking all the characters used in the text\n",
    "char = sorted(list(set(text)))\n",
    "vocab_size = len(char)\n",
    "print(''.join(char))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bringing in tiktoken's tokenizer\n",
    "\n",
    "# THIS WILL NOT WORK\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373]\n",
      "[818, 616, 7099, 290, 517, 8826, 812, 616, 2988, 2921, 502, 617, 5608, 326, 314, 447, 247, 303, 587, 6225, 625, 287]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In my younger and more vulnerable years my father gave me some advice that I’ve been turning over in'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing with encoding and decoding; it works now, but note that we only have\n",
    "# a vocab_size of 85\n",
    "encode = enc.encode(\"hello\")\n",
    "print(encode)\n",
    "enc.decode(encode)\n",
    "\n",
    "encode1 = enc.encode(text[:100])\n",
    "print(encode1)\n",
    "enc.decode(encode1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54, 63, 63]\n",
      "foo\n"
     ]
    }
   ],
   "source": [
    "# hence we need to tokenize the vocab ourselves\n",
    "stoi = { ch:i for i, ch in enumerate(char)}\n",
    "itos = { i:ch for i, ch in enumerate(char)}\n",
    "\n",
    "def encode(str):\n",
    "    return [stoi[c] for c in str]\n",
    "\n",
    "def decode(data):\n",
    "    return \"\".join(itos[c] for c in data)\n",
    "\n",
    "foo = encode(\"foo\")\n",
    "print(foo)\n",
    "print(decode(foo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 62, 0, 61, 73, 0, 73, 63, 69, 62, 55, 53, 66, 0, 49, 62, 52, 0, 61, 63, 66, 53, 0, 70, 69, 60, 62, 53, 66, 49, 50, 60, 53, 0, 73, 53, 49, 66, 67, 0, 61, 73, 0, 54, 49, 68, 56, 53, 66, 0, 55, 49, 70, 53, 0, 61, 53, 0, 67, 63, 61, 53, 0, 49, 52, 70, 57, 51, 53, 0, 68, 56, 49, 68, 0, 30, 82, 70, 53, 0, 50, 53, 53, 62, 0, 68, 69, 66, 62, 57, 62, 55, 0, 63, 70, 53, 66, 0, 57, 62]\n",
      "In my younger and more vulnerable years my father gave me some advice that I’ve been turning over in\n"
     ]
    }
   ],
   "source": [
    "# testing encode with my own functions\n",
    "# notice the difference between tiktoken\n",
    "# but we cannot use tiktoken so this will do\n",
    "test_encode = encode(text[:100])\n",
    "print(test_encode)\n",
    "print(decode(test_encode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([270151]) torch.int64\n",
      "tensor([30, 62,  0, 61, 73,  0, 73, 63, 69, 62, 55, 53, 66,  0, 49, 62, 52,  0,\n",
      "        61, 63, 66, 53,  0, 70, 69, 60, 62, 53, 66, 49, 50, 60, 53,  0, 73, 53,\n",
      "        49, 66, 67,  0, 61, 73,  0, 54, 49, 68, 56, 53, 66,  0, 55, 49, 70, 53,\n",
      "         0, 61, 53,  0, 67, 63, 61, 53,  0, 49, 52, 70, 57, 51, 53,  0, 68, 56,\n",
      "        49, 68,  0, 30, 82, 70, 53,  0, 50, 53, 53, 62,  0, 68, 69, 66, 62, 57,\n",
      "        62, 55,  0, 63, 70, 53, 66,  0, 57, 62,  0, 61, 73,  0, 61, 57, 62, 52,\n",
      "         0, 53, 70, 53, 66,  0, 67, 57, 62, 51, 53,  8,  0,  0, 83, 44, 56, 53,\n",
      "        62, 53, 70, 53, 66,  0, 73, 63, 69,  0, 54, 53, 53, 60,  0, 60, 57, 59,\n",
      "        53,  0, 51, 66, 57, 68, 57, 51, 57, 74, 57, 62, 55,  0, 49, 62, 73, 63,\n",
      "        62, 53,  6, 84,  0, 56, 53,  0, 68, 63, 60, 52,  0, 61, 53,  6,  0, 83,\n",
      "        58, 69, 67, 68,  0, 66, 53, 61, 53, 61, 50, 53, 66,  0, 68, 56, 49, 68,\n",
      "         0, 49, 60, 60,  0, 68, 56, 53,  0, 64, 53, 63, 64, 60, 53,  0, 57, 62,\n",
      "         0, 68, 56, 57, 67,  0, 71, 63, 66, 60, 52,  0, 56, 49, 70, 53, 62, 82,\n",
      "        68,  0, 56, 49, 52,  0, 68, 56, 53,  0, 49, 52, 70, 49, 62, 68, 49, 55,\n",
      "        53, 67,  0, 68, 56, 49, 68,  0, 73, 63, 69, 82, 70, 53,  0, 56, 49, 52,\n",
      "         8, 84,  0,  0, 29, 53,  0, 52, 57, 52, 62, 82, 68,  0, 67, 49, 73,  0,\n",
      "        49, 62, 73,  0, 61, 63, 66, 53,  6,  0, 50, 69, 68,  0, 71, 53, 82, 70,\n",
      "        53,  0, 49, 60, 71, 49, 73, 67,  0, 50, 53, 53, 62,  0, 69, 62, 69, 67,\n",
      "        69, 49, 60, 60, 73,  0, 51, 63, 61, 61, 69, 62, 57, 51, 49, 68, 57, 70,\n",
      "        53,  0, 57, 62,  0, 49,  0, 66, 53, 67, 53, 66, 70, 53, 52,  0, 71, 49,\n",
      "        73,  6,  0, 49, 62, 52,  0, 30,  0, 69, 62, 52, 53, 66, 67, 68, 63, 63,\n",
      "        52,  0, 68, 56, 49, 68,  0, 56, 53,  0, 61, 53, 49, 62, 68,  0, 49,  0,\n",
      "        55, 66, 53, 49, 68,  0, 52, 53, 49, 60,  0, 61, 63, 66, 53,  0, 68, 56,\n",
      "        49, 62,  0, 68, 56, 49, 68,  8,  0, 30, 62,  0, 51, 63, 62, 67, 53, 65,\n",
      "        69, 53, 62, 51, 53,  6,  0, 30, 82, 61,  0, 57, 62, 51, 60, 57, 62, 53,\n",
      "        52,  0, 68, 63,  0, 66, 53, 67, 53, 66, 70, 53,  0, 49, 60, 60,  0, 58,\n",
      "        69, 52, 55, 53, 61, 53, 62, 68, 67,  6,  0, 49,  0, 56, 49, 50, 57, 68,\n",
      "         0, 68, 56, 49, 68,  0, 56, 49, 67,  0, 63, 64, 53, 62])\n"
     ]
    }
   ],
   "source": [
    "# now encoding the entire text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training data and validation data\n",
    "n = int(0.9*len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30, 62,  0, 61, 73,  0, 73, 63, 69])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up block size\n",
    "block_size = 8\n",
    "train[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting batch size\n",
    "batch_size = 4\n",
    "\n",
    "# function for getting a batch of random blocks within data, set my batch_size\n",
    "def get_batch(split):\n",
    "    if split == \"train\":\n",
    "        data = train\n",
    "    else:\n",
    "        data = val\n",
    "\n",
    "    ix = torch.randint(len(data) - batch_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8, 84,  0,  0, 30,  0, 71, 49],\n",
      "        [69, 66,  0, 71, 57, 54, 53,  6],\n",
      "        [63,  0, 68, 56, 53,  0, 54, 66],\n",
      "        [30,  0, 61, 49, 52, 53,  0, 68]])\n",
      "tensor([[84,  0,  0, 30,  0, 71, 49, 67],\n",
      "        [66,  0, 71, 57, 54, 53,  6, 84],\n",
      "        [ 0, 68, 56, 53,  0, 54, 66, 63],\n",
      "        [ 0, 61, 49, 52, 53,  0, 68, 56]])\n"
     ]
    }
   ],
   "source": [
    "# collecting inputs and targets from the training data\n",
    "# targets used for creating the loss function later on\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "print(xb)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the language model\n",
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.token_embedding_table(inputs) # batch, time, channel\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # need to reformat BTC into B*C, T for loss to work\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c)\n",
    "\n",
    "            # targets are in B T and needs to be B*T\n",
    "            targets = targets.view(b*t)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, number):\n",
    "        for _ in range(number):\n",
    "            logits, loss = self(inputs)\n",
    "            logits = logits[:, -1, :]\n",
    "            prob = F.softmax(logits, 1)\n",
    "            inputs_next = torch.multinomial(prob, 1)\n",
    "            inputs = torch.cat((inputs, inputs_next), 1)\n",
    "\n",
    "        return inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we would expect a loss of -ln(1/86), which is approximately **-4.45**, but we are getting almost 5 right now. This means the inital predictions are not very diffused yet, and there is entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 86])\n",
      "tensor(5.0984, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# this was where i found that tiktoken wouldn't work\n",
    "model = BigramLM(vocab_size)\n",
    "\n",
    "logits, loss = model(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks silly now because history is not used; we only examine the last character in time: **logits = logits[:, -1, :]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ixgé6a2Bbp8)8]vKEOW—;…7::*[’1WExQ:r,……OA…;gh:C9N3Gs”—E êRF.9çWô.‘8WRh78V‘Jw’‘2)bTGfL“5“6a;ISmHm:EaqE\n"
     ]
    }
   ],
   "source": [
    "# setting an input as a 1 by 1 tensor of zeros\n",
    "input = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(input, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a pytorch optimzer object\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model.**\n",
    "We can see the loss going down everytime we train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m): \u001b[38;5;66;03m# my computer almost blew up\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn[32], line 13\u001b[0m, in \u001b[0;36mget_batch\u001b[1;34m(split)\u001b[0m\n\u001b[0;32m     11\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m batch_size, (batch_size,))\n\u001b[0;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i:i \u001b[38;5;241m+\u001b[39m block_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m---> 13\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m: i \u001b[38;5;241m+\u001b[39m block_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# increasing batch size and setting a loop to evaluate loss\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(10000): # my computer almost blew up\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Step {i}: Loss = {loss.item()}\")\n",
    "\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are drastic improvements than to before.\n",
    "\n",
    "\n",
    "\"—;84c7.nd]ontxe$4neHLKyéxUa—fW]’g0IySCPjoM3j,4R 0h—$é ],Oé4x$érBXJHP12MGay8?W’O[ ?[3xIV9é?*NC “va1G\" \n",
    "\n",
    "to\n",
    "\n",
    "\n",
    "\" d I wnt harsurthed is owemeveg wayouio id aly a this wicy’se  d Minghe Shouad yoofezzive rnan y dineng o thiniss mecegs wayth inn r  “Antoke!” s ge, motthof, ay wes blut cemomos. astr I wa dad Wes dyecakneitlastat ceche. “Ale nidosef My I roufol amed cousy ay?”  s, od. son’tanonghat athad Routheed d\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " oum I tsperd and, herd h f t g I “o hanon I h bey, bomoonugol athe. ain oe t. “Dop fow ooane an an bofe te m be. hakn. ainy Plloryor dedinthe thon’sp. y y in cerachthe drdaly, ar ived sshedst s sil lye ce dr wadrld sinjurie oper g ancedio iexe ubringr “Tot, “I aitheler. Itre. Thag. thafo … Wed a knd\n"
     ]
    }
   ],
   "source": [
    "# testing the generate function\n",
    "input = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(model.generate(input, 300)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
